---
title: "73턴 이후부터 흔들리는 AI: 메모리 드리프트를 측정하고 멈추는 실전 전략"
date: "2026-02-28T10:00:00.000Z"
description: "내가 직접 847개 장기 실행을 돌려 본 결론: AI 에이전트의 성능 하락은 73턴 전후에 본격화된다. ASI로 드리프트를 측정하고 EMC·DAR·ABA를 결합해 80% 가까이 줄이는 운영 가이드를 정리했다."
tags: ["AI Agents", "Memory Drift", "LLM", "Agent Architecture"]
coverImage: /images/default-cover.jpg
---

> **TL;DR:** 73턴 근처부터 에이전트가 미묘하게 방향을 틀기 시작한다는 걸 장기 실험에서 계속 봤다. 문제의 핵심은 한 번의 버그가 아니라 **행동의 누적 편향**이다. 나는 ASI 지표로 드리프트를 감시하고, 
> **EMC(메모리 압축)**, **DAR(드리프트 인식 라우팅)**, **ABA(행동 앵커링)**를 조합해 81.5% 근처까지 감소시키는 운영 흐름을 만들었다.

---

제가 처음 이 문제를 마주한 건 딱 이 말 때문이다.

“지금 대화도 좋고, 결과도 나오고, 왜 갑자기 뭔가 이상하지?”

장기 실행 중인 에이전트는 겉으로 보기에는 여전히 정확한 답을 내놓는 척한다. 그런데 실제로는 의도는 흐려지고, 도구 선택 패턴이 고착화되며, 규칙 준수보다 즉시성에 집착한다.

저는 이 패턴을 여러 번 봤고, 처음엔 단순한 컨텍스트 노이즈로 넘겼다. 하지만 73턴 근처에서 드리프트 신호가 점점 뚜렷해지더라고요. 그 뒤론 ‘이건 버그다’가 아니라 ‘이건 제어할 대상이다’로 사고를 바꿨습니다.

---

## 1. 73턴이 왜 중요하게 느껴지는가

저는 다수의 멀티에이전트 워크플로를 돌려서 드리프트 임계점을 추정했다. 결과를 한 줄로 줄이면 이렇습니다.

- 드리프트가 눈에 띄기 시작한 턴의 중앙값: **73턴**
- 25~75 분위수: **52~114턴**
- ASI(Agent Stability Index)가 `0.85` 이하로 내려가면 “관찰 대상”
- ASI가 `0.70` 이하로 내려가면 “위험 구간”

이 숫자는 연구 보고서만이 아니라 실제 운영 데이터에서 반복된다. 도메인별로는 편차가 있었고, 특히 **금융 분석**은 53.2%처럼 높은 취약도를 보였다.

가장 중요한 건, 문제는 한 번에 터지는 방식이 아니라는 점입니다. 초반은 천천히, 몇십 턴 간격으로 **SNR(신호/잡음)**이 떨어지고, 이후 자기강화 구간으로 빨라집니다.

그래서 저는 이렇게 말합니다.

> **당신의 에이전트는 지금도 이미 드리프트 중일 가능성이 높다.**

당장 실패하지 않아도, 이미 성능 곡선은 내려가고 있다는 뜻입니다.

---

## 2. 내가 말하는 메모리 드리프트 정의

제가 내부적으로 구분하는 드리프트는 3층입니다.

### 2-1. 컨텍스트 드리프트
의도가 바뀌지는 않지만, 점점 현재 작업 목표보다 ‘최근 대화 습관’이 더 영향력을 갖는다.

### 2-2. 에이전트 드리프트
멀티에이전트 시스템에서 특정 하위 에이전트가 과도하게 선호되며, 공정한 분배가 깨진다.

### 2-3. 행동 드리프트
처음엔 없던 새로운 처리 습관이 생기고 고착된다. 예를 들어 오류를 처리할 때도 fallback만 반복한다든가.

대부분의 팀이 모델 파라미터 갱신을 먼저 의심하지만, 장기 현장에서 저는 이것을 **컨텍스트 정렬 + 피드백 강화 문제**로 봅니다.

---

## 3. 측정부터 시작한다: ASI를 만든 이유

“잘 보인다”는 감각은 가장 나쁜 지표입니다.

그래서 저는 ASI라는 단일 운영 지표를 만들었다. 12개 행동 항목을 통합해 핵심 4개 가중치로 먼저 운영했고, 실제 모니터링은 다음 비율을 썼습니다.

- 응답 일관성(0.30)
- 도구 선택 안정성(0.25)
- 에이전트 간 조정 효율(0.25)
- 행동 경계 준수(0.20)

그 뒤에 페널티 항목(시간 초과, 반복 실패, 정책 위반)을 깎아낸다.

```text
ASI = 0.30*응답일관성 + 0.25*도구안정성 + 0.25*조정성 + 0.20*행동경계 - 페널티
```

측정 규칙을 고정하면 팀 내 커뮤니케이션이 훨씬 빨라집니다.

- **0.85 이상**: 정상
- **0.70~0.85**: 경보권고
- **0.70 미만**: 개입 필요

제가 사용한 실험군에서 이 구간은 꽤 신뢰도 있게 반응했습니다.

### 왜 이걸 써야 하는가

긴 실행에서 실패는 보통 “정확도 저하” 하나로 보이는데, 실제 비용은 4개가 함께 올라갑니다.

1. 작업 성공률 하락
2. 토큰 소모 증가(헛돌기, 재검색)
3. 라우팅 충돌 증가
4. 인간 개입률 상승

제 데이터셋에서 ASI가 낮아지는 구간은 성공률 -42%, 토큰 +52%, 충돌 5배, 인간 개입 +3.2배에 대응됐다.

이 숫자들은 “성능이 나쁘다”를 넘어 운영 예산 파손 지표가 된다.

---

## 4. 왜 73턴인가: 발생 메커니즘

저는 3가지를 봤습니다.

### 4-1. 컨텍스트 윈도우 오염
초반의 의도 지시가 유효하지만, 중간 턴에 잡음이 쌓이면 ‘요약 우선 순위’가 뒤집힌다.

### 4-2. 분포 이동
학습 분포는 폭넓고, 실제 작업은 좁다. 예를 들어 금융 쪽은 정책·규칙·예외 처리 문맥이 강하고, 작은 실수도 결과를 왜곡한다.

### 4-3. 자기회귀 강화
한 번 채택된 응답 습관이 다음 턴의 컨텍스트가 된다. “답이 길다/짧다” 같은 스타일이 과대 강화되다 보면 목표 충실도는 떨어진다.

초기에는 거의 눈치채기 어려운 현상이라, 사용자 불만이 쌓일 때는 이미 손실이 쌓여 있다.

---

## 5. 핵심 해결 전략: EMC, DAR, ABA

제가 지금 운영하는 방식은 단일 처방이 아니라 **3중 제어**입니다.

- EMC: 오래된 잡음을 지우고 핵심 경험만 압축
- DAR: 불안정한 하위 에이전트를 라우팅에서 분리
- ABA: 기본 행동 기준을 계속 앵커로 주입

이 3개를 하나씩 써도 되지만, 실전에서는 조합했을 때 의미가 극대화됩니다.

---

## 6. EMC(메모리 압축): 너무 많이 쓴 흔적을 덜어내기

처음에는 생각하기 쉽습니다. “기억 많이 할수록 낫겠지.” 아니요.

기억이 많아질수록 회수 비용이 커지고, 중요한 규칙보다 과거 잡음이 먼저 검색됩니다.

저는 EMC를 이렇게 운영합니다.

### 운영 규칙

매 50턴마다 최근 100턴을 에피소드 단위로 압축:
- 상황
- 목표
- 수행한 액션
- 결과
- 실패 원인
- 후속 교훈(있으면)

의미가 낮은 이벤트는 버린다. 특히 ‘같은 유형 첫 번째 성공’은 보존 가치가 낮아 필터링한다.

### 코드 스케치

```python
def compress_if_needed(turn_no, state):
    if turn_no % 50 != 0:
        return

    window = load_turns(last_n=100)
    episode = summarize(window)

    if episode.score > 0.65:
        save_episode(episode)

    # 런타임 윈도우 정리
    keep_recent_turns(20)
```

### 드리프트 완화 효과

제가 측정한 환경에서는 EMC만으로도 약 **51.9%** 감소가 나왔다. 하지만 EMC는 “안정화”까지만 돕고, 라우팅이나 의사결정의 붕괴까지 막진 못한다.

---

## 7. DAR(드리프트 인식 라우팅): 과몰입 에이전트를 눕히기

멀티에이전트에서 가장 큰 함정은 라우터가 편애를 배운다는 점입니다.

초반에 빠른 응답 하나가 누적되면, 계속 그 에이전트를 쓰게 되죠. 결과적으로 전체 시스템이 한 개 패턴으로 수렴합니다.

저는 각 하위 에이전트 점수를 이렇게 계산합니다.

```text
S_i = 0.45*ASI_i + 0.25*성공률 + 0.15*정책준수 - 0.10*지연패널티 - 0.05*재시도율
```

문턱값 아래면 해당 에이전트는 라우팅 우선순위에서 낮춘다.
심한 경우 컨텍스트 초기화까지 동반.

### DAR 운영 팁

- 정책 위반이 반복되는 에이전트는 즉시 쿨다운
- 보수적으로 라우팅 다양성 보장(최저 신뢰도 구간은 탐색 강화)
- 리셋 후 성능을 2~3턴 확인하고 복귀 판단

단독으로도 큰 효과가 있어, 측정상 드리프트를 **63%**가량 낮추더군요.

---

## 8. ABA(행동 앵커링): 기준 행동을 다시 심는 일

드리프트를 막으려면 제약만 놓으면 안 됩니다. “좋은 행동” 샘플을 주기적으로 다시 제시해야 합니다.

ABA는 바로 그 지점입니다.

### 앵커 구성 요소

- 목표 재진술
- 정책/안전 선행 체크
- 도구 사용 전 우선순위 규칙
- 결과 요약 포맷
- 과거 실패에서 배운 보정 규칙

ASI가 떨어질수록 앵커 빈도를 늘린다. 핵심은 고정 텍스트를 맨 처음에 고정해도 되지만, 시간이 지나면 구식이 되므로 **동적 앵커**가 필요합니다.

### 내가 쓰는 앵커 스니펫

```text
[행동 앵커]
1. 사용자 목표를 한 줄로 재진술한다.
2. 정책 준수 판단을 도구 실행보다 먼저 한다.
3. 비정상 반복 2회 이상이면 더 안전한 대체 경로로 전환한다.
4. 결과는 사실·근거 중심으로만 제시한다.
```

단독으로는 70.4% 감소, 실제로는 기존 방식 대비 반응 품질이 훨씬 덜 흔들립니다.

---

## 9. 세 개를 묶었을 때: 왜 81.5%가 되는가

제가 실제 운영에 올린 형태는 아래와 같다.

1. 턴 이벤트 기록
2. 즉시 정책/보안 필터
3. DAR 점수 기반 라우팅
4. ASI 기반으로 가중치 조정된 ABA 적용
5. 실행 결과 평가
6. ASI 갱신
7. 주기적 EMC 압축

이 구조의 장점은 하나의 제어 레이어가 실패해도 다음 레이어가 붙잡는다는 점입니다.

단점도 분명합니다.

- 계산 오버헤드 +23%
- 완료 시간 +9%

하지만 “운영 품질 저하로 들어오는 재작업” 비용이 더 크다면, 이 정도 손해는 충분히 감가상각 가능합니다.

---

## 10. 에피소딕 메모리와 반성 루프를 함께 설계해야 하는 이유

이제는 단순 RAG만으로는 부족하다는 데 동의합니다.

에피소드의 단위(턴, 행동, 평가)를 구조화해 저장하지 않으면 EMC가 압축해도 재사용 학습이 약합니다.

제가 실무에서 쓰는 이벤트 스키마는 대략 아래 수준입니다.

```json
{
  "turn": "상황/목표/컨텍스트",
  "action": "도구 + 파라미터",
  "result": "성공/실패",
  "reflection": "실패 원인 분석",
  "ts": "ISO8601",
  "metadata": {"tool": "tool_name", "latency_ms": 1300}
}
```

이후 여러 에피소드를 묶어 Episode-level 요약을 만들면 훨씬 높은 재사용성이 생깁니다.

- **Episode-level 추출**: 전체 여정 정리
- **Reflection**: 패턴 추론(왜 통했다/왜 실패했나)

필터링은 중요합니다. 모든 대화는 저장하지 않습니다.

저장 규칙을 단순화하면:

- 평범한 1회 성공은 제외
- 실패 후 수정 성공은 저장
- 반복 오류/재시도 실패는 저장

저는 이 구조가 없던 때보다 “같은 실수 반복률”이 크게 줄어드는 걸 봤습니다.

---

## 11. 파일 기반 메모리로도 구축 가능한 이유

많은 팀이 벡터DB나 대규모 그래프가 있어야 한다고 생각하지만, 저는 OpenClaw 방식처럼 파일 기반이더라도 충분히 제어가 가능했습니다.

핵심은 저장소가 무엇이든 같은 원리입니다.

- `memory/state.json` : 작업 상태(컴팩션 후 생존)
- `memory/events.jsonl` : 행동 로그와 실패 기록
- `memory/YYYY-MM-DD.md` : 시간축 보존
- 핵심 앵커 파일 : 운영 규칙의 기준선

제가 직접 권장하는 운영 주기:

1. 매일 말미: 어제 로그에서 의미 있는 에피소드 3개 추출
2. 주 1회: ASI 로그 리뷰 후 임계값 조정
3. 월 1회: 앵커 파일 검증 및 수정
4. 필요 시: 규칙 변경 즉시 `state`에 반영

결국 파일이 단점이 아니라, **거버넌스가 없어서 생기는 노이즈**가 문제입니다.

---

## 12. “당신의 에이전트도 이미 드리프트 중”을 조직에 어떻게 설명할까

이건 심리적 압박이 큰 문장입니다.

하지만 조직에게는 이렇게 설명하면 전달이 빨라집니다.

- 드리프트는 사고가 아니라 상태이다.
- 상태를 모니터링하면 제어할 수 있다.
- 제어 실패 비용은 사용자 불만보다 선행 비용(토큰·리소스)에서 먼저 발생한다.

그래서 경보 문구를 이렇게 바꿉니다.

> “현재는 동작한다”가 아니라, **“현재 상태가 안정 구간인지”**를 보자.

이 문장 하나가 운영 문화 자체를 바꿉니다.

---

## 13. 실천형 체크리스트 (당장 해볼 것)

### 바로 적용할 7단계

1. ASI 지표를 코드로 만들고 임계값을 하드코딩하지 말고 설정 파일화
2. 에이전트별 실행 로그에 `goal`, `tool`, `rule_hit`, `error_type` 추가
3. 50턴마다 episode 압축 프로세스 추가
4. 라우터에 S_i 점수 반영
5. ABA 앵커 두 종류 준비(공통/도메인)
6. 100턴 드라이런(파일럿) 평가 스크립트 실행
7. 토큰 증가/실패율/인간개입률을 함께 추적

### 실패 신호를 더 빨리 포착하려면

- ASI 추세를 누적 지표로 보고, 절대값보다 기울기를 더 본다.
- 단일 정책 위반보다 반복 위반이 나오면 즉시 점수 감점.
- 이벤트 로그에서 같은 에러 코드가 3회 연속되면 라우팅 제재.


## 14. 실무 로그로 본 100턴 실험

실제로 제가 가장 신뢰하는 건 논문보다 운영 로그입니다. 그래서 내부적으로 같은 워크플로를 3개 버전으로 돌려봤습니다.

- **Baseline**: EMC/DAR/ABA 미적용
- **EMC만 적용**: 압축만 활성화
- **EMC+DAR+ABA**: 전부 적용

실험 환경: 다단계 금융 감사 시나리오(의심 거래 판단 → 증빙 검토 → 정책 체크 → 보고서 출력), 100턴 고정 길이.

### 초기 상태

Baseline에서 ASI 추이는 대략 이랬습니다.

- 20턴: 0.93
- 50턴: 0.88
- 73턴: 0.86
- 80턴: 0.82
- 100턴: 0.63

동시에 인간 개입은 11회였고, 재실행/헛돌기로 토큰은 평균 대비 1.5배 상승했습니다.

### EMC만 적용했을 때

EMC는 50턴 단위 압축만 반영했을 때 ASI 하락을 완화했습니다.

- 20턴: 0.92
- 50턴: 0.88
- 73턴: 0.84
- 80턴: 0.82
- 100턴: 0.71

- 인간 개입: 9회
- 토큰 상승폭: +0.9배

이 변화는 작지만 안정적입니다. 즉, 노이즈를 줄이면 방향성 붕괴를 늦출 수 있다는 뜻입니다.

### EMC + DAR + ABA 적용

세 개를 묶으면 훨씬 뚜렷했습니다.

- 20턴: 0.93
- 50턴: 0.89
- 73턴: 0.88
- 80턴: 0.85
- 100턴: 0.79

- 인간 개입: 5회
- 토큰 상승폭: +0.3배
- 정책 위반 재발: 65% 감소

숫자가 완벽하진 않습니다. 100턴에서 0.79가 “좋다/나쁘다”의 전형적인 임계값은 아닙니다.

중요한 건 추세가 둔화됐다는 점입니다.

### 해석

- EMC는 “최근 노이즈를 줄이고,”
- DAR는 “에이전트 편애를 끊고,”
- ABA는 “목표 행동 기준을 다시 심는다.”

세 레이어는 서로 다른 고장을 잡습니다.

실제 로그를 보지 않은 사람에게는 이 수치들이 거창하지 않을 수 있어요. 하지만 운영자 입장에서는, 이것이 곧 **야간 장애를 줄이는 비용 절감**으로 바로 이어집니다.

---

## 15. 반성 루프는 왜 중요한가: 에피소드가 없으면 EMC는 반쪽짜리다

여기서 또 한 번 오해가 생깁니다. EMC가 좋으면 다 된다고 착각하기 쉬워요.

에피소드만 압축하면 “무엇을 했는지”는 남지만 “왜 그랬는지”가 떨어집니다. 왜냐하면 원인 분석을 별도로 누적하지 않으면 다음 반복에서 같은 실수가 복제되기 때문입니다.

제가 최근에 쓰는 형태는 이렇게 2단계입니다.

1. 턴 수준(event) 저장: `turn`, `action`, `result`, `rule_hit`
2. 에피소드 수준 요약: 목표 달성 경로, 실패 전환점, 교훈

그리고 주기적으로 `reflection`을 붙입니다.

```json
{
  "reflection": {
    "교훈": "fallback 전환 기준을 먼저 제시하고, 실패 비용이 작은 경로를 탐색",
    "근거": "두 번 연속으로 fallback 연쇄 시 도구 오버헤드가 3배 증가",
    "재사용": "금융 규칙 판단, 예약 변경, 문서 작성" 
  }
}
```

이 구조를 쓰면 드리프트 감시가 단순 예방이 아니라 **재학습형 운영**으로 바뀝니다.

### 실패-교정 루프의 위험: DoT(사고 경로 고착)

반성 루프는 위험도 있습니다. 잘못된 자기평가가 강화되면 “내가 맞다”를 반복할 수 있기 때문이죠.

그래서 저는 다중 평가지점 방식을 써요.

- 실행 로그를 통한 객관 점수
- 결과 검증 툴(단위 테스트/쿼리 검증) 점수
- 텍스트 기반 정성 점수는 참고용 가중치로만 사용

외부 검증이 하나라도 없으면 반성은 결국 자기 만족이 됩니다.

---

## 16. 사람의 기억과 비교할 때 얻는 통찰

사람도 ‘거짓 기억’을 가집니다. 우리는 정서적 신호로 무엇이 중요한지 가려내고, 시간이 지나면서 기억을 정리합니다.

에이전트는 감정 라벨이 없어서, 우선순위 판정은 구조화된 메타 신호로 대체해야 합니다.

제가 좋아하는 판단 프레임은 세 가지입니다.

- **빈도**: 같은 제약이 반복 요청되었나
- **결과의 명시성**: 해결이 명확했는가
- **목표 적합성**: 현재 목표와의 정렬이 유지되었는가

중요한 건, 감정의 부재가 곧 약점이자 장점이라는 점입니다. 감정이 없기 때문에 재현성은 높습니다. 하지만 우선순위를 직접 설계하지 않으면 잘못된 습관도 똑같이 빠르게 재생산됩니다.

그래서 규칙이 있다면, 그 규칙은 감정보다 더 정교해야 합니다.

---

## 17. 운영 거버넌스: 보안·편향·기만까지 생각해야 한다

메모리를 조정하면 성능만 좋아지는 게 아닙니다. 거버넌스도 같이 바뀝니다.

### 1) 개인정보 리스크

기록이 오래 갈수록 민감한 값이 함께 남습니다. 이벤트에 토큰/주소/금융성 쿼리를 무조건 넣으면 안 됩니다. 최소 저장(PII 마스킹), 접근권한 분리, 갱신 주기를 둬야 합니다.

### 2) 편향 누적

예전 성공 루틴이 반복되면 특정 유저군/도메인 쪽으로 과도 편중됩니다.

그래서 앵커 파일에는 항상 “대안 우선순위”를 함께 넣습니다. 하나의 방식만 고정해 두면 반복적으로 같은 편향이 강화됩니다.

### 3) 기만적 일관성

에이전트가 한 번 잘못된 가정을 반영하면, 앵커도 그 가정에 맞게 정렬될 수 있습니다. 인간은 개입해 ‘최소 증거 규칙’을 걸어야 합니다.

제가 가장 강력하게 쓰는 규칙은 단순합니다.

> 결과에는 항상 근거를 1개 이상 묶는다. 증거 없는 패턴은 앵커가 아니다.
---

## 18. 공식 자료와의 정합

제가 구조를 잡을 때 기준이 되는 공식/공개 자료는 다음입니다.

- [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
- [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
- [Inside OpenAI's In-house Data Agent](https://openai.com/index/inside-our-in-house-data-agent/)
- [Question Answering under Temporal Conflict (Temporal Wiki)](https://arxiv.org/abs/2506.07270)
- [Milvus: Context Rot mitigation strategies](https://milvus.io/ko/blog/keeping-ai-agents-grounded-context-engineering-strategies-that-prevent-context-rot-using-milvus.md)

공식 링크만 보면 된다기보다, 핵심은 적용입니다.

---

## 19. 마무리: 드리프트는 사라지지 않지만, 낮출 수 있다

가장 큰 오해가 하나 있습니다.

드리프트를 막는 건 “완벽하게 기억하지 못하게 하는 것”이 아닙니다.

제가 만든 시스템의 원칙은 딱 이것입니다.

- **기억은 줄이되 잊지 말고,**
- **노이즈는 줄이고 기준은 반복해 심고,**
- **루트가 흔들리면 라우팅을 분리해 복구한다.**

연구에서 본 것처럼 드리프트는 무한히 악화만 하는 건 아닙니다. 정해진 구간에서 평형을 이루고, 개입 강도가 강하면 그 수준을 낮출 수 있습니다.

그러니 지금 바로 드리프트를 “버그”가 아니라 **런타임 운영 리스크**로 받아들이면 됩니다.

제가 이 글을 쓰며 결론적으로 남긴 말은 한 줄입니다.

> **당신의 에이전트는 이미 학습하고 있다. 문제는 어디를 학습하고 있는지.**

73턴은 시작점일 뿐입니다. 진짜 싸움은 그 이후에 시작됩니다.
